{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources\n",
    "Documentations\n",
    "https://mpi4py.readthedocs.io/en/stable/tutorial.html\n",
    "\n",
    "Princeton Computing (this includes how to do it  on real clusters, but if you are just running it on PC, you can just look at the example code here)\n",
    "https://researchcomputing.princeton.edu/mpi4py\n",
    "\n",
    "Other sources (these videos are great)\n",
    "https://www.youtube.com/watch?v=CT9tqR7XeX0&list=PLQVvvaa0QuDf9IW-fe6No8SCw-aVnCfRi&index=14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To install on PC\n",
    "On linux: (tested on ubuntu 18.04)\n",
    "\n",
    "easiest just do it through conda (or anaconda if you have installed the anaconda-navigator, go to environment page and just install the packages), \n",
    "first get openMPI, then get the mpi4py\n",
    "\n",
    "for reference and alternative installation methods, check these links:\n",
    "\n",
    "https://pypi.org/project/mpi4py/\n",
    "\n",
    "https://pythonprogramming.net/installing-testing-mpi4py-mpi-python-tutorial/\n",
    "\n",
    "https://anaconda.org/anaconda/mpi4py\n",
    "\n",
    "#### To run code: (OPENMPI)\n",
    "mpiexec -n 8 python Script.py (this will run Script.py on 8 slots in parallel)\n",
    "\n",
    "### If running on a PC and want the program to run on threads instead of cores as \"slot\", use the command similiar to this example below\n",
    "\n",
    "mpiexec --use-hwthread-cpus -n 16 python Script.py\n",
    "\n",
    "instad of \n",
    "\n",
    "mpiexec -n 8 python Script.py\n",
    "\n",
    "for an 8 core 16 threads CPU\n",
    "\n",
    "check this links for more controls over the \"slots\":https://github.com/open-mpi/ompi/issues/6020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, test if the block below works. (Hint, it should!)\n",
    "running each block will generate a python script file. Then use the bash command given above to run it on how ever many slots you want, just changed the filenames you want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Script_Parallel_Testing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Script_Parallel_Testing.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "   data = [(x+1)**x for x in range(size)]\n",
    "   print ('we will be scattering:',data)\n",
    "else:\n",
    "   data = None\n",
    "   \n",
    "data = comm.scatter(data, root=0)\n",
    "print ('rank',rank,'has data:',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Script_Parallel_Testing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Script_Parallel_Testing.py\n",
    "# hello_mpi.py:\n",
    "# usage: python hello_mpi.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "\n",
    "def print_hello(rank, size, name): \n",
    "#rank = processor index, size = # of processors, name = cluster name\n",
    "  msg = \"Hello World! I am process {0} of {1} on {2}.\\n\"\n",
    "  sys.stdout.write(msg.format(rank, size, name))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  size = MPI.COMM_WORLD.Get_size()\n",
    "  rank = MPI.COMM_WORLD.Get_rank()\n",
    "  name = MPI.Get_processor_name()\n",
    "\n",
    "  print_hello(rank, size, name)\n",
    "\n",
    "#printouts: 5 tasks, 1/cpu\n",
    "#Hello World! I am process 1 of 5 on DESKTOP-HV1REH2.\n",
    "#Hello World! I am process 3 of 5 on DESKTOP-HV1REH2.\n",
    "#Hello World! I am process 2 of 5 on DESKTOP-HV1REH2.\n",
    "#Hello World! I am process 0 of 5 on DESKTOP-HV1REH2.\n",
    "#Hello World! I am process 4 of 5 on DESKTOP-HV1REH2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below are tutorials\n",
    "\n",
    "Some of these examples are taken and modified from online examples, such as those given in the resources mentioned above.\n",
    "\n",
    "### Basic Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Script_Parallel_Testing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Script_Parallel_Testing.py\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD #can pull name, rank, size etc. from this. This object is the barebone basic of MPI\n",
    "print('Hi, my rank is:', comm.rank)\n",
    "if comm.rank == 1: #if this i the node 1\n",
    "    print('doing the task for rank(node) 1')\n",
    "elif comm.rank == 0: \n",
    "    print('doing the task for rank(node) 0')\n",
    "elif comm.rank == 2:\n",
    "    print('doing the task for rank(node) 2')\n",
    "\n",
    "print('finishing node %i \\n\\n'%comm.rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Script_Parallel_Testing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Script_Parallel_Testing.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD #can pull name, rank, size etc. from this\n",
    "rank = comm.rank\n",
    "size = comm.size #so that we dont need to hard code size into our code\n",
    "print('Hi, my rank is:', rank)\n",
    "print('rank^size is ',rank**size)\n",
    "print('finishing node %i \\n'%rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Script_Parallel_Testing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Script_Parallel_Testing.py\n",
    "from mpi4py import MPI\n",
    "import time\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "size = comm.size\n",
    "name = MPI.Get_processor_name() #the() at the end!\n",
    "\n",
    "shared1 = 'shared 1'#(rank+1)*5 #the message to be shared\n",
    "shared2 = 'shared 2'\n",
    "if rank == 1:\n",
    "    data_from_0 = comm.recv(source=0)\n",
    "    data_from_0pt2 = comm.recv(source=0)\n",
    "    print('on rank',rank,name,'we received data=',data_from_0,data_from_0pt2)\n",
    "if rank == 0: #0 usually master node\n",
    "    data_to_1 = shared1\n",
    "    data_to_1pt2 = shared2\n",
    "    time.sleep(2) #sleep for 2 senconds\n",
    "    comm.send(data_to_1,dest=1) #sharing data from R0 to R1\n",
    "    comm.send(data_to_1pt2,dest=1)\n",
    "    #comm.send(DATA_TO_BE_SENT,DESTINATION_RANK_#)\n",
    "    print('from rank',rank,name,'we sent data=',data_to_1,data_to_1pt2)\n",
    "\n",
    "####PRINTOUTS\n",
    "#from rank 0 DESKTOP-HV1REH2 we sent data= shared 1 shared 2\n",
    "#on rank 1 DESKTOP-HV1REH2 we received data= shared 1 shared 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So regardless if we code receive or send first, the receiver will wait\n",
    "The data will be received in order. \n",
    "- If a node is waiting for a data thta is never sent, then the task will run indefinitely, with full CPU Utilization (more on tagging below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamically send and receive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Script_Parallel_Testing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Script_Parallel_Testing.py\n",
    "from mpi4py import MPI\n",
    "import time\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "size = comm.size\n",
    "name = MPI.Get_processor_name()\n",
    "\n",
    "shared = [rank,rank**2*np.pi]\n",
    "comm.send(shared,dest = (rank+1)%size) #send to the next worker/node\n",
    "data = comm.recv(source = (rank-1)%size) #receive from the previous worker/node\n",
    "print(rank, name)\n",
    "print('sent data = ', shared, ' to rank ',(rank+1)%size)\n",
    "print('received = ', data, ' from rank ',(rank-1)%size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging\n",
    "compare the two below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Script_Parallel_Testing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Script_Parallel_Testing.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "\n",
    "if rank == 0:\n",
    "    shared1 = {'d1':1,'d2':2}\n",
    "    comm.send(shared1,dest = 1)\n",
    "    shared2 = {'d1':999,'d2':9999}\n",
    "    comm.send(shared2,dest = 1)\n",
    "if rank == 1:\n",
    "    receive1 = comm.recv(source=0)\n",
    "    print(receive1)\n",
    "    print(receive1['d1'])\n",
    "    receive2 = comm.recv(source=0)\n",
    "    print(receive2)\n",
    "    print(receive2['d1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Script_Parallel_Testing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Script_Parallel_Testing.py\n",
    "# But if we dont know the order of the shared msg that are sent out\n",
    "# and we still want to differentiate them?\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "\n",
    "if rank == 0:\n",
    "    shared1 = {'d1':1,'d2':2}\n",
    "    comm.send(shared1, dest=1, tag=1)\n",
    "    shared2 = {'d1':999,'d2':9999}\n",
    "    comm.send(shared2, dest=1, tag=2)\n",
    "if rank == 1:\n",
    "    receive2 = comm.recv(source=0, tag=2)\n",
    "    print(receive2)\n",
    "    print(receive2['d1'])\n",
    "    receive1 = comm.recv(source=0, tag=1)\n",
    "    print(receive1)\n",
    "    print(receive1['d1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BroadCasting, the msg is sent to all the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Script_Parallel_Testing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Script_Parallel_Testing.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "\n",
    "if rank == 0: #create data onn master node\n",
    "    data_0 = {'a':1,'b':2,'c':3}\n",
    "else:\n",
    "    data_0 = None #set data = None on all other nodes\n",
    "    print('rank',rank,'before receiving, has ',data_0)\n",
    "data_0 = comm.bcast(data_0, root=0)\n",
    "\n",
    "\n",
    "print('rank ',rank,' has data = ',data_0)\n",
    "if rank == 0:\n",
    "    print('this is rank 0, I have both', data_0,data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Script_Parallel_Testing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Script_Parallel_Testing.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "\n",
    "if rank == 0: #create data onn master node\n",
    "    data_0 = {'a':1,'b':2,'c':3}\n",
    "else:\n",
    "    #set data = None on all other nodes\n",
    "    data_0 = None \n",
    "    #this is very important!! \n",
    "    #the argument variable in bcast need to be declared on all nodes before Bcast\n",
    "    pass\n",
    "print('before Bcast, rank',rank,'has data',data_0)\n",
    "\n",
    "data_0 = comm.bcast(data_0,root=0)\n",
    "print('after Bcast, rank',rank,'has data',data_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter and Gather\n",
    "\n",
    "Scatter: take a list, explode it in pieces to nodes\n",
    "\n",
    "then 'gather' gathers the exploded pieces of information and reassembles them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Script_Parallel_Testing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Script_Parallel_Testing.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = np.array([(x+1) for x in range(size)]) #so at least one obj per node\n",
    "    #data = np.append(data,[9,9,9,9]) #this wont work, can only scatter # = size for this command\n",
    "    print('we will be scattering:',data)\n",
    "else:\n",
    "    data = None\n",
    "\n",
    "data_scat = comm.scatter(data,root=0)\n",
    "print('rank',rank,'has scattered data = ', data_scat)\n",
    "if rank == 0:\n",
    "    print('this is rank 0')\n",
    "    print('data = ',data)\n",
    "    print('data_scat = ',data_scat)\n",
    "\n",
    "data_scat = data_scat*2\n",
    "dataNew = comm.gather(data_scat,root=0)\n",
    "if rank == 0:\n",
    "    print('this is rank 0, dataNew = ', dataNew)\n",
    "print(rank,'check',dataNew)#for other non-0 nodes, this will report None\n",
    "print(rank,'check',data_scat)#the nodes still retained the gathered data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU LOAD TESTINGS\n",
    "cat /proc/cpuinfo | grep processor | wc -l \n",
    "gives # of cores but not nec. cpu\n",
    "\n",
    "max slots can get for example on a Ryzen 3700X are 8 if in cores, although it has 16 Hthreads which you can call by the second command below\n",
    "\n",
    "#### Normally:\n",
    "mpiexec -n 8 python CPU_Load_5s_Testing.py\n",
    "\n",
    "#### If wanting to run a task/HThread, then use the command:\n",
    "mpiexec --use-hwthread-cpus -n 16 python CPU_Load_5s_Testing.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CPU_Load_5s_Testing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile CPU_Load_5s_Testing.py \n",
    "#this is about 5 sec, but depends on the computer you are working on of course. Although should not be too long\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "for i in range(500000000):\n",
    "    if i>=-1:\n",
    "        pass\n",
    "print('rank = ',rank,'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
